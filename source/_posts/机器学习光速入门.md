---
title: 机器学习光速入门
date: 2018-08-19 09:59:46
tags: [机器学习]
---

# 机器学习光速入门

`原创作者：法狗狗测试工程师 - 泰斯特`

***

## 有监督学习

***

### 线性回归 (梯度下降算法)

自学了一阵子的机器学习，也算是小有收获，先从最简单的线性回归开始走起 :)

话说有一天你走在大街上，路过一家房产中介，玻璃窗上标满了各类型房产的价格，你在感叹工作十年都买不起一个厕所的同时也在思考，房子的价格是由哪些因素决定的呢 ? (ps: 房子为什么那么贵呢？？？) 一般情况下，房子越大，价格越贵，那么可以认为**房子的面积**算是影响房子价格的一个因素。假设房子的价格仅由**房子的面积**、**房间的数量**决定，那么房子价格的线性表达式即为:


$$
h_\theta(x) = \theta_0 + \theta_1x_{房子的面积} + \theta_2x_{房间的数量}
$$
$\theta$ 表示各个特征的权重 ，其中 $\theta_0$ 表示截距项(常数项) ， 当特征数量庞大时，我们可以用以下表达式来简化线性方程:


$$
h_\theta(x) = \sum_{i=0}^{n}\theta_ix_i = \theta^Tx
$$
其中 $n$ 代表特征的数量， $x_0 = 1$ 并且右式中 $\theta$ 和 $x$ 为向量表达式


至此 , 我们的假设方程已经诞生了！那么问题来了，我们要如何去选择 $\theta$ 呢？
为了解决这个问题，我们定义**损失函数**为:

$$
J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2
$$

其中 $m$ 代表 **训练样本的数量**、 $x^{(i)}$代表第 $i$ 个**训练样本的特征**、$y^{(i)}$代表第 $i$ 个**训练样本的目标**， 不难看出，损失函数的值越小，**假设**与**目标**越接近

这时候，我们可以用**梯度下降算法**来找出能够**最小化损失函数**的 $\theta$

先介绍一下什么是**梯度**:

	梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。 				---- 百度百科

通俗点说，**梯度就是表示函数最大变化方向的向量**，对于一元函数来说，梯度就是该函数的导数

那么什么是**梯度下降**呢？梯度下降就是说**函数在当前点的超切面上沿着梯度负方向下降速率最快**，那么这是为什么呢？ 请看以下推导过程:

![梯度下降原理推导](https://img-blog.csdn.net/20180820115133194?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTkwODY0OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 也就是说根据下列式子即可得到我们想要的 $\theta$


$$
\theta_j := \theta_j - \alpha_{学习速率}\frac{\partial}{\partial\theta_j}J(\theta)
$$
那么上式中的 $\frac{\partial}{\partial\theta_j}J(\theta)$ 怎么求呢？

$$
\frac{\partial}{\partial\theta_j}J(\theta) = \frac{\partial}{\partial\theta_j}\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2
$$
$$
= \sum_{i=1}^{m}\frac{1}{2}\frac{\partial}{\partial\theta_j}(h_\theta(x^{(i)}) - y^{(i)})^2
$$
$$
= \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})\frac{1}{2}\cdot 2 \frac{\partial}{\partial\theta_j} (\sum_{j=0}^{n}\theta_jx^{(i)}_j - y^{(i)})
$$
$$
= \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
$$
最后我们得到**批量梯度下降算法**:


 **Repeat until convergent{**  
$$\theta_j := \theta_j - \alpha\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(for every j)**
**}**



当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解。在这里，
$J(\theta)$ 是凸函数，所以我们可以获得**全局最优解**   (假设学习速率$\alpha$ 足够小)

根据批量梯度下降算法每次权重迭代都需要对整个训练集进行运算 (**速度慢！**)，所以当训练集特别大的时候我们可以考虑选择**随机梯度下降算法**:

**Loop{**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for i=1 to m, {**

$$
\theta_j := \theta_j - \alpha(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**(for every j)**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**}**
**}**
可以看出，**随机梯度下降**每次权重迭代仅需要对**单个训练样本**进行运算，速度上对比**批量梯度下降**要提升不少，虽然通过随机梯度下降不一定能保证 $\theta$ 收敛 (在$J(\theta)$最小值处波动)，但是基本上可以获得一个接近最小值的近似值
至此，相信大家对梯度下降算法有了初步的认识，我们下一章见
